---
title: "RWorksheet5_(Celestra, Caneso, Arenal)"
output: pdf_document
date: "2024-11-06"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
**[EXTRACTING TV SHOWS REVIEWS]**
```{r}
```
**1.**
```{r}
```
**________________________________________________________**
**Libraries**
```{r}
library(rvest)
library(httr)
library(polite)
library(dplyr)
library(stringr)
library(knitr) 
```

*IMDB URL*
```{r}
polite::use_manners(save_as = 'polite_scrape.R')

url <- "https://www.imdb.com/chart/toptv/?ref_=nv_tvv_250"

session <- bow(url,
user_agent = "Educational")
session

```

*Title List*
```{r}
title_list <- scrape(session) %>%
html_nodes('h3.ipc-title__text') %>%
html_text

title_list_sub <- as.data.frame(title_list[2:26])
title_list_sub
```

*Split*
```{r}
colnames(title_list_sub) <- "ranks"

split_df <- strsplit(as.character(title_list_sub$ranks),".",fixed = T)
split_df <- data.frame(do.call(rbind,split_df))

colnames(split_df) <- c("ranks", "titles")
split_df
```

*Rating*
```{r}
rating_list <- scrape(session) %>%
html_nodes('span.ipc-rating-star--rating') %>%
html_text

```

*Number of People who voted*
```{r}
people_list <- scrape(session) %>%
html_nodes('span.ipc-rating-star--voteCount') %>%
html_text

```

*Episode and Year it was released*
```{r}
episode_list <- scrape(session) %>%
html_nodes('span.sc-5bc66c50-6.OOdsw.cli-title-metadata-item:nth-of-type(2)') %>%
html_text


year_list <- scrape(session) %>%
html_nodes('span.sc-5bc66c50-6.OOdsw.cli-title-metadata-item:nth-of-type(1)') %>%
html_text

```

**User Reviews, Critic Reviews, and Popularity Rating**
```{r}
home_link <- 'https://www.imdb.com/chart/toptv/'
main_page <- read_html(home_link)

links <- main_page %>%
  html_nodes("a.ipc-title-link-wrapper") %>%
  html_attr("href")

# Loop to get link of each show's page
show_data <- lapply(links, function(link) {
  complete_link <- paste0("https://imdb.com", link)
  
  #loop to get the link for user review page
  usrv_link <- read_html(complete_link)
  usrv_link_page <- usrv_link %>%
    html_nodes('a.isReview') %>%
    html_attr("href")
  
  #loop to get user reviews of each shows
  usrv <- read_html(paste0("https://imdb.com", usrv_link_page[1]))
  usrv_count <- usrv %>%
    html_nodes('[data-testid="tturv-total-reviews"]') %>%
    html_text()
  
  #loop to extract critic reviews
  critic <- usrv_link %>%
              html_nodes("span.score") %>%
              html_text()
  critic_df <- data.frame(Critic_Reviews = critic[2], stringsAsFactors = FALSE)
  
  #loop to extract pop rating
  pop_rating <- usrv_link %>%
              html_nodes('[data-testid="hero-rating-bar__popularity__score"]') %>%
              html_text()
  pop_rating_df <- data.frame(Popularity_Rating = pop_rating[2], stringsAsFactors = FALSE)
  
  return(data.frame(User_Reviews = usrv_count, Critic = critic_df, pop = pop_rating_df)) 
})

showss <- do.call(rbind, show_data)
```

**FINAL DATA**
```{r}
tv_shows <- data.frame(Ranking = split_df$ranks,
                       TV_Show_Title = split_df$titles,
                       Episodes = episode_list[1:25],
                       Release_Year = year_list[1:25],
                       Rating = rating_list[1:25],
                       No._of_Voter = people_list[1:25],
                       User_Review = showss$User_Reviews,
                       Popularity = showss$Popularity_Rating,
                       Critic_Reviews = showss$Critic_Reviews)
tv_shows
```
**________________________________________________________**
```{r}
#just for space
```

**2. Top 5 TV Shows**
```{r}


urls <- c("https://www.imdb.com/title/tt0903747/reviews/?ref_=tt_urv_sm",
          "https://www.imdb.com/title/tt5491994/reviews/?ref_=tt_urv_sm",
          "https://www.imdb.com/title/tt0795176/reviews/?ref_=tt_urv_sm",
          "https://www.imdb.com/title/tt0185906/reviews/?ref_=tt_urv_sm",
          "https://www.imdb.com/title/tt7366338/reviews/?ref_=tt_urv_sm")

df <- list()

for(i in seq_along(urls)){
  
  sessions <- bow(urls[i], user_agent = "Educational")

#User name
userName <- scrape(sessions) %>%
html_nodes('a.ipc-link.ipc-link--base') %>%
html_text() %>%
  head(20)

#Date reviewed
dateReview <- scrape(sessions) %>%
html_nodes('li.ipc-inline-list__item.review-date') %>%
html_text() %>%
  head(20)

#User rating
userRating <- scrape(sessions) %>%
html_nodes('span.ipc-rating-star--rating') %>%
html_text() %>%
  head(20)

#Title of the review
reviewTitle <- scrape(sessions) %>%
html_nodes('h3.ipc-title__text') %>%
html_text() %>%
  head(20)

#Helpful
helpful <- scrape(sessions) %>%
html_nodes('span.ipc-voting__label') %>%
html_text()

#Unhelpful
unhelpful <- scrape(sessions) %>%
html_nodes('span.ipc-voting__label__count.ipc-voting__label__count--down') %>%
html_text() 

#Data frame for the user reviews.
userReviews <- data.frame(user_Name = userName[1:20],
                          Date_Reviewed = dateReview[1:20],
                          user_Rating = userRating[1:20],
                          Review_Title = reviewTitle[1:20],
                          Helpful = helpful[1:20],
                          Unhelpful = unhelpful[1:20],
                          stringsAsFactors = F)

df[[i]] <- userReviews
}

df[[1]]
df[[2]]
df[[3]]
df[[4]]
df[[5]]
```
**________________________________________________________**
```{r}
#just for space
```

**3.**
```{r}
#just for space
```



**_______________________________________________________________________________________________**
```{r}
#just for space
```


**[EXTRACTING AMAZON PRODUCT REVIEWS]**
```{r}
library(rvest)
library(dplyr)

amazonCategory <- c(
  'https://www.amazon.com/s?k=graphics+card&crid=6809YKENO5VZ&sprefix=graphics+%2Caps%2C516&ref=nb_sb_ss_ts-doa-p_1_9',
  'https://www.amazon.com/s?k=laptop&crid=15FICF43OPKJ2&sprefix=laptop%2Caps%2C1359&ref=nb_sb_ss_ts-doa-p_1_6',
  'https://www.amazon.com/s?k=pc+cases&crid=3232B9XX8J30L&sprefix=pccase%2Caps%2C674&ref=nb_sb_ss_ts-doa-p_3_6',
  'https://www.amazon.com/s?k=gaming+chair&crid=1QD90SHFVLWF3&sprefix=ganing%2Caps%2C529&ref=nb_sb_ss_w_hit-vc-lth_gaming-chair_k0_1_6',
  'https://www.amazon.com/s?k=desk&crid=6858WNPPCX45&sprefix=desk%2Caps%2C420&ref=nb_sb_ss_w_hit-vc-lth_desk_k0_1_4'
)

amazon <- list()

for (i in seq_along(amazonCategory)) {
  Sys.sleep(5)  # Avoid overwhelming the server
  
  session2 <- bow(amazonCategory[i], user_agent = "Educational")
  
  # Scrape data
  product_titles <- scrape(session2) %>%
    html_nodes("span.a-text-normal") %>% 
    html_text() %>%
    head(30)
  product_titles <- product_titles[!grepl("Check each product page for other buying options", product_titles)]
  
  price <- scrape(session2) %>% 
    html_nodes('.a-price .a-offscreen') %>% 
    html_text() %>%
    head(30)
  
  ratings <- scrape(session2) %>% 
    html_nodes('span.a-icon-alt') %>% 
    html_text() %>%
    head(30)
  
  reviews <- scrape(session2) %>%
    html_nodes('.s-link-style .s-underline-text') %>% 
    html_text() %>%
    head(30)
  
  descriptions <- scrape(session2) %>%
    html_nodes('.a-row.a-size-base.a-color-secondary') %>% 
    html_text() %>%
    head(30)
  
  # Find the maximum length
  max_length <- max(
    length(product_titles), length(price), length(ratings),
    length(reviews), length(descriptions)
  )
  
  # Standardize all vectors to the same length
  length(product_titles) <- max_length
  length(price) <- max_length
  length(ratings) <- max_length
  length(reviews) <- max_length
  length(descriptions) <- max_length
  
  # Create a data frame
  amaz <- data.frame(
    ProductTitle = product_titles[1:16],
    Price = price[1:16],
    Ratings = ratings[1:16],
    Reviews = reviews[1:16],
    Description = descriptions[1:16],
    stringsAsFactors = FALSE
  )
  
amazon[[i]] <- amaz
}

```

```{r}
# Combine all data frames into one
amazon[[1]]
amazon[[2]]
```

**Graphics Crad**
```{r}

gc_url <- c("https://www.amazon.com/s?k=graphics+card&crid=6809YKENO5VZ&sprefix=graphics+%2Caps%2C516&ref=nb_sb_ss_ts-doa-p_1_9",
            "https://www.amazon.com/s?k=graphics+card&page=2&crid=NY6LX0H811DR&qid=1732062550&sprefix=%2Caps%2C1816&ref=sr_pg_2")

graphics <- list()

for(i in seq_along(gc_url)){
  
  gSession <- bow(amazonCategory[i], user_agent = "Educational")
  
  # Scrape data
  product_titles <- scrape(gSession) %>%
    html_nodes("span.a-text-normal") %>% 
    html_text() %>%
    head(30)
  product_titles <- product_titles[!grepl("Check each product page for other buying options", product_titles)]
  
  price <- scrape(gSession) %>% 
    html_nodes('.a-price .a-offscreen') %>% 
    html_text() %>%
    head(30)
  
  ratings <- scrape(gSession) %>% 
    html_nodes('span.a-icon-alt') %>% 
    html_text() %>%
    head(30)
  
  reviews <- scrape(gSession) %>%
    html_nodes('.s-link-style .s-underline-text') %>% 
    html_text() %>%
    head(30)
  
  descriptions <- scrape(gSession) %>%
    html_nodes('.a-row.a-size-base.a-color-secondary') %>% 
    html_text() %>%
    head(30)
  
  # Find the maximum length
  max_length <- max(
    length(product_titles), length(price), length(ratings),
    length(reviews), length(descriptions)
  )
  
  # Standardize all vectors to the same length
  length(product_titles) <- max_length
  length(price) <- max_length
  length(ratings) <- max_length
  length(reviews) <- max_length
  length(descriptions) <- max_length
  
  # Create a data frame
  card <- data.frame(
    ProductTitle = product_titles[1:15],
    Price = price[1:15],
    Ratings = ratings[1:15],
    Reviews = reviews[1:15],
    Description = descriptions[1:15],
    stringsAsFactors = FALSE
  )
  
graphics[[i]] <- card
}
```

```{r}
Gcard <- rbind(graphics[[1]], graphics[[2]])

kable(Gcard, caption = "Graphics Card Category")
```

